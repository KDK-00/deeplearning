{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZi61kH4renPksbB4Fp760",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KDK-00/deeplearning/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.데이터 로드 및 전처리**"
      ],
      "metadata": {
        "id": "tVkSFWk0gjvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "프로그램을 위한 패키지들을 import한다"
      ],
      "metadata": {
        "id": "EHMTtPhPZr2m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XA2XJWqt1wIp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "링크 : http://www.manythings.org/anki\n",
        "\n",
        "Fra - Eng 병렬 코퍼스된 데이터를 사용"
      ],
      "metadata": {
        "id": "v_wGlYAUZ0Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#전체 데이터중 33000개의 데이터를 사용\n",
        "num_samples = 33000"
      ],
      "metadata": {
        "id": "drlCbVQ92BHy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c http://www.manythings.org/anki/fra-eng.zip && unzip -o fra-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81shxU9h2C39",
        "outputId": "3b35ce18-130c-4bf4-a102-f4bb265bf050"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-17 03:08:52--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  24.5MB/s    in 0.3s    \n",
            "\n",
            "2024-05-17 03:08:53 (24.5 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 전처리 함수를 구현"
      ],
      "metadata": {
        "id": "ND3C-4NTc27y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unicode_to_ascii(s):\n",
        "  # 프랑스어 악센트(accent) 삭제\n",
        "  # 예시 : 'déjà diné' -> deja dine\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
      ],
      "metadata": {
        "id": "7N3Xs89f25RH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sent):\n",
        "  # 악센트 삭제 함수 호출\n",
        "  sent = unicode_to_ascii(sent.lower())\n",
        "\n",
        "  # 단어와 구두점 사이에 공백을 만듭니다.\n",
        "  # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
        "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "  # 다수 개의 공백을 하나의 공백으로 치환\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "  return sent\n"
      ],
      "metadata": {
        "id": "fNuRW2kK3Wfi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # source 데이터와 target 데이터 분리\n",
        "      src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      # source 데이터 전처리\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target 데이터 전처리\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in)\n",
        "      decoder_target.append(tar_line_out)\n",
        "\n",
        "      if i == num_samples - 1:\n",
        "        break\n",
        "\n",
        "  return encoder_input, decoder_input, decoder_target\n"
      ],
      "metadata": {
        "id": "o85X2_0I3Yui"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 테스트\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', en_sent)\n",
        "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
        "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
        "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqGZzNzg3alT",
        "outputId": "f9bd4edc-355f-4548-a957-89cfe12af8a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 전 영어 문장 : Have you had dinner?\n",
            "전처리 후 영어 문장 : have you had dinner ?\n",
            "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
            "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "구두점과 공백이 적절하게 전처리되어 출력됨"
      ],
      "metadata": {
        "id": "ziM7UO0Kc_Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()"
      ],
      "metadata": {
        "id": "QnvEl3pe3gsb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
        "print('인코더의 입력 :',sents_en_in[:5])\n",
        "print('디코더의 입력 :',sents_fra_in[:5])\n",
        "print('디코더의 레이블 :',sents_fra_out[:5])"
      ],
      "metadata": {
        "id": "PipBkBTr3pF6",
        "outputId": "0165afbd-ca10-4406-9b07-23aa94d36e25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
            "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
            "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 과정에서는 디코더의 이전 셀 출력이 아닌 실제 타겟값(sents_fra_in)을 현재 셀의 입력으로 사용하는 교사 강요(Teacher Forcing) 기법을 적용함\n",
        "\n",
        "이것은 잘못된 예측값의 연쇄 전파로 인한 성능 저하를 방지하고 효율적인 훈련을 위해서임"
      ],
      "metadata": {
        "id": "WrTRP7M2eJgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어로부터 정수를 얻는 딕셔너리\n",
        "\n",
        "단어집합을 구현"
      ],
      "metadata": {
        "id": "-BMg1Ooveo4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sents):\n",
        "  word_list = []\n",
        "\n",
        "  for sent in sents:\n",
        "      for word in sent:\n",
        "        word_list.append(word)\n",
        "\n",
        "  # 각 단어별 등장 빈도를 계산하여 등장 빈도가 높은 순서로 정렬\n",
        "  word_counts = Counter(word_list)\n",
        "  vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "  word_to_index = {}\n",
        "  word_to_index['<PAD>'] = 0\n",
        "  word_to_index['<UNK>'] = 1\n",
        "\n",
        "  # 등장 빈도가 높은 단어일수록 낮은 정수를 부여\n",
        "  for index, word in enumerate(vocab) :\n",
        "    word_to_index[word] = index + 2\n",
        "\n",
        "  return word_to_index\n"
      ],
      "metadata": {
        "id": "2w8Hp5A83rXL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "build_vocab은 입력된 데이터로부터 단어의 등장 빈도순으로 정렬 후에 등장 빈도가 높은 순서일 수록 낮은 정수를 부여함\n",
        "\n",
        "이때, 패딩 토큰을 위한 <PAD> 토큰은 0번, OOV에 대응하기 위한 <UNK> 토큰은 1번에 할당. 이렇게 되면 빈도수가 가장 높은 단어는 정수가 2번, 빈도수가 두번 째로 많은 단어는 정수 3번이 할당됨."
      ],
      "metadata": {
        "id": "RBPZQpb5e2l5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = build_vocab(sents_en_in)\n",
        "tar_vocab = build_vocab(sents_fra_in + sents_fra_out)\n",
        "\n",
        "src_vocab_size = len(src_vocab)\n",
        "tar_vocab_size = len(tar_vocab)\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAnwzaWofMEK",
        "outputId": "d3b87222-f4a5-41e0-a3aa-153bf18458d7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 단어 집합의 크기 : 4486, 프랑스어 단어 집합의 크기 : 7879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어를 위한 단어 집합 src_vocab과 프랑스어를 이용한 단어 집합 tar_vocab"
      ],
      "metadata": {
        "id": "om7RmuThfMs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_src = {v: k for k, v in src_vocab.items()}\n",
        "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
        "\n",
        "def texts_to_sequences(sents, word_to_index):\n",
        "  encoded_X_data = []\n",
        "  for sent in tqdm(sents):\n",
        "    index_sequences = []\n",
        "    for word in sent:\n",
        "      try:\n",
        "          index_sequences.append(word_to_index[word])\n",
        "      except KeyError:\n",
        "          index_sequences.append(word_to_index['<UNK>'])\n",
        "    encoded_X_data.append(index_sequences)\n",
        "  return encoded_X_data\n"
      ],
      "metadata": {
        "id": "NF65MicefPKb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = texts_to_sequences(sents_en_in, src_vocab)\n",
        "decoder_input = texts_to_sequences(sents_fra_in, tar_vocab)\n",
        "decoder_target = texts_to_sequences(sents_fra_out, tar_vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90YIUIYlfeD7",
        "outputId": "a5f0fd8b-f316-4266-b403-feaa10c6ec2f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33000/33000 [00:00<00:00, 138836.46it/s]\n",
            "100%|██████████| 33000/33000 [00:00<00:00, 340117.98it/s]\n",
            "100%|██████████| 33000/33000 [00:00<00:00, 441250.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 상위 5개의 샘플에 대해서 정수 인코딩 전, 후 문장 출력\n",
        "# 인코더 입력이므로 <sos>나 <eos>가 없음\n",
        "for i, (item1, item2) in zip(range(5), zip(sents_en_in, encoder_input)):\n",
        "    print(f\"Index: {i}, 정수 인코딩 전: {item1}, 정수 인코딩 후: {item2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGO2Re-sfgbk",
        "outputId": "997f1118-c21a-415b-e24b-e3a6c7833adf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 0, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 1, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 2, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 3, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 4, 정수 인코딩 전: ['hi', '.'], 정수 인코딩 후: [736, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences(sentences, max_len=None):\n",
        "    # 최대 길이 값이 주어지지 않을 경우 데이터 내 최대 길이로 패딩\n",
        "    if max_len is None:\n",
        "        max_len = max([len(sentence) for sentence in sentences])\n",
        "\n",
        "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
        "    for index, sentence in enumerate(sentences):\n",
        "        if len(sentence) != 0:\n",
        "            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "f2Snswj6fiGo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = pad_sequences(encoder_input)\n",
        "decoder_input = pad_sequences(decoder_input)\n",
        "decoder_target = pad_sequences(decoder_target)\n"
      ],
      "metadata": {
        "id": "54SoUxv7fkrX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
        "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
        "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c85YnUzQfmTx",
        "outputId": "3dab2eb4-bda0-4e20-e210-505d81b5b940"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력의 크기(shape) : (33000, 7)\n",
            "디코더의 입력의 크기(shape) : (33000, 16)\n",
            "디코더의 레이블의 크기(shape) : (33000, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 데이터를 분리하기 전 데이터를 섞어줌\n",
        "\n",
        "이를 위해 순서가 섞인 정수 시퀀스 리스트를 생성"
      ],
      "metadata": {
        "id": "0kwtXei1fvtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print('랜덤 시퀀스 :',indices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xt38O76foBe",
        "outputId": "bbb7b044-dc9e-43db-87dd-3f65a339eef2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "랜덤 시퀀스 : [25962 20390 25995 ... 29022  7022 17564]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]\n"
      ],
      "metadata": {
        "id": "cHPlMvF7frK4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "임의로 30,997번째 샘플을 출력"
      ],
      "metadata": {
        "id": "RfQ0HlwQf_FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([index_to_src[word] for word in encoder_input[30997]])\n",
        "print([index_to_tar[word] for word in decoder_input[30997]])\n",
        "print([index_to_tar[word] for word in decoder_target[30997]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyizQtWGf4Zw",
        "outputId": "6b17a77c-45bf-4404-b357-0f9d8a236515"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['who', 'spoke', 'french', '?', '<PAD>', '<PAD>', '<PAD>']\n",
            "['<sos>', 'qui', 'parlait', 'francais', '?', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['qui', 'parlait', 'francais', '?', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전체 데이터의 수 33,000 의 10%인 3,300개를 테스트 데이터로 사용"
      ],
      "metadata": {
        "id": "zemGQ0zYgINd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_val = int(33000*0.1)\n",
        "print('검증 데이터의 개수 :',n_of_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd0sYswzf6Dq",
        "outputId": "5f087cfd-ee92-4389-bc77-4c3fc595caac"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 데이터의 개수 : 3300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]\n"
      ],
      "metadata": {
        "id": "scd2yBKEgDhD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
        "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
        "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
        "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
        "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
        "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjy7wCnjgRJ-",
        "outputId": "d16e2c2c-a52e-4c50-fb30-ca420fbaefee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 source 데이터의 크기 : (29700, 7)\n",
            "훈련 target 데이터의 크기 : (29700, 16)\n",
            "훈련 target 레이블의 크기 : (29700, 16)\n",
            "테스트 source 데이터의 크기 : (3300, 7)\n",
            "테스트 target 데이터의 크기 : (3300, 16)\n",
            "테스트 target 레이블의 크기 : (3300, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.기계 번역기 만들기**"
      ],
      "metadata": {
        "id": "fqrShKZUgfPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "embedding_dim = 256\n",
        "hidden_units = 256\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embedding_dim, hidden_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape == (batch_size, seq_len, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        # hidden.shape == (1, batch_size, hidden_units), cell.shape == (1, batch_size, hidden_units)\n",
        "        _, (hidden, cell) = self.lstm(x)\n",
        "        # 인코더의 출력은 hidden state, cell state\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, tar_vocab_size, embedding_dim, hidden_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(tar_vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_units, tar_vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "\n",
        "        # x.shape == (batch_size, seq_len, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # 디코더의 LSTM으로 인코더의 hidden state, cell state를 전달.\n",
        "        # output.shape == (batch_size, seq_len, hidden_units)\n",
        "        # hidden.shape == (1, batch_size, hidden_units)\n",
        "        # cell.shape == (1, batch_size, hidden_units)\n",
        "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
        "\n",
        "        # output.shape: (batch_size, seq_len, tar_vocab_size)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        # 디코더의 출력은 예측값, hidden state, cell state\n",
        "        return output, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # 훈련 중에는 디코더의 출력 중 오직 output만 사용한다.\n",
        "        output, _, _ = self.decoder(trg, hidden, cell)\n",
        "        return output\n",
        "\n",
        "encoder = Encoder(src_vocab_size, embedding_dim, hidden_units)\n",
        "decoder = Decoder(tar_vocab_size, embedding_dim, hidden_units)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n"
      ],
      "metadata": {
        "id": "Z-MxwzxegZAV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY9UXXSKgzy7",
        "outputId": "81ccd5e1-def9-4cd0-c8de-e15e7975ddb8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(4486, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, batch_first=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(7879, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, batch_first=True)\n",
            "    (fc): Linear(in_features=256, out_features=7879, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**모델부분 수정필요**"
      ],
      "metadata": {
        "id": "pKw0lG8sg-vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "평가함수 구현"
      ],
      "metadata": {
        "id": "R4xKq1FShGa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model, dataloader, loss_function, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n",
        "            encoder_inputs = encoder_inputs.to(device)\n",
        "            decoder_inputs = decoder_inputs.to(device)\n",
        "            decoder_targets = decoder_targets.to(device)\n",
        "\n",
        "            # 순방향 전파\n",
        "            # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n",
        "            outputs = model(encoder_inputs, decoder_inputs)\n",
        "\n",
        "            # 손실 계산\n",
        "            # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n",
        "            # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n",
        "            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # 정확도 계산 (패딩 토큰 제외)\n",
        "            mask = decoder_targets != 0\n",
        "            total_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item()\n",
        "            total_count += mask.sum().item()\n",
        "\n",
        "    return total_loss / len(dataloader), total_correct / total_count\n"
      ],
      "metadata": {
        "id": "H2WUQAQYg3yW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n",
        "decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n",
        "decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n",
        "\n",
        "encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n",
        "decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n",
        "decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n",
        "\n",
        "# 데이터셋 및 데이터로더 생성\n",
        "batch_size = 128\n",
        "\n",
        "train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "gmkXS65MhEBy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 설정\n",
        "num_epochs = 30\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxPwVkZphKf4",
        "outputId": "53b2e91d-1e9e-49a2-f81e-3408c3de7324"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(4486, 256, padding_idx=0)\n",
              "    (lstm): LSTM(256, 256, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(7879, 256, padding_idx=0)\n",
              "    (lstm): LSTM(256, 256, batch_first=True)\n",
              "    (fc): Linear(in_features=256, out_features=7879, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # 훈련 모드\n",
        "    model.train()\n",
        "\n",
        "    for encoder_inputs, decoder_inputs, decoder_targets in train_dataloader:\n",
        "        encoder_inputs = encoder_inputs.to(device)\n",
        "        decoder_inputs = decoder_inputs.to(device)\n",
        "        decoder_targets = decoder_targets.to(device)\n",
        "\n",
        "        # 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 순방향 전파\n",
        "        # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n",
        "        outputs = model(encoder_inputs, decoder_inputs)\n",
        "\n",
        "        # 손실 계산 및 역방향 전파\n",
        "        # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n",
        "        # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n",
        "        loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # 가중치 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device)\n",
        "    valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n",
        "\n",
        "    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n",
        "\n",
        "    # 검증 손실이 최소일 때 체크포인트 저장\n",
        "    if valid_loss < best_val_loss:\n",
        "        print(f'Validation loss improved from {best_val_loss:.4f} to {valid_loss:.4f}. 체크포인트를 저장합니다.')\n",
        "        best_val_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vsWKoB7hTbw",
        "outputId": "f76c601b-f9d1-4987-fafe-eefb3c06d0ca"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30 | Train Loss: 2.9337 | Train Acc: 0.5274 | Valid Loss: 3.0091 | Valid Acc: 0.5318\n",
            "Validation loss improved from inf to 3.0091. 체크포인트를 저장합니다.\n",
            "Epoch: 2/30 | Train Loss: 2.2821 | Train Acc: 0.6011 | Valid Loss: 2.4883 | Valid Acc: 0.5949\n",
            "Validation loss improved from 3.0091 to 2.4883. 체크포인트를 저장합니다.\n",
            "Epoch: 3/30 | Train Loss: 1.8777 | Train Acc: 0.6442 | Valid Loss: 2.1982 | Valid Acc: 0.6242\n",
            "Validation loss improved from 2.4883 to 2.1982. 체크포인트를 저장합니다.\n",
            "Epoch: 4/30 | Train Loss: 1.5730 | Train Acc: 0.6817 | Valid Loss: 2.0036 | Valid Acc: 0.6485\n",
            "Validation loss improved from 2.1982 to 2.0036. 체크포인트를 저장합니다.\n",
            "Epoch: 5/30 | Train Loss: 1.3276 | Train Acc: 0.7141 | Valid Loss: 1.8623 | Valid Acc: 0.6649\n",
            "Validation loss improved from 2.0036 to 1.8623. 체크포인트를 저장합니다.\n",
            "Epoch: 6/30 | Train Loss: 1.1091 | Train Acc: 0.7553 | Valid Loss: 1.7434 | Valid Acc: 0.6869\n",
            "Validation loss improved from 1.8623 to 1.7434. 체크포인트를 저장합니다.\n",
            "Epoch: 7/30 | Train Loss: 0.9332 | Train Acc: 0.7841 | Valid Loss: 1.6640 | Valid Acc: 0.6934\n",
            "Validation loss improved from 1.7434 to 1.6640. 체크포인트를 저장합니다.\n",
            "Epoch: 8/30 | Train Loss: 0.7755 | Train Acc: 0.8205 | Valid Loss: 1.5868 | Valid Acc: 0.7074\n",
            "Validation loss improved from 1.6640 to 1.5868. 체크포인트를 저장합니다.\n",
            "Epoch: 9/30 | Train Loss: 0.6476 | Train Acc: 0.8487 | Valid Loss: 1.5404 | Valid Acc: 0.7133\n",
            "Validation loss improved from 1.5868 to 1.5404. 체크포인트를 저장합니다.\n",
            "Epoch: 10/30 | Train Loss: 0.5488 | Train Acc: 0.8684 | Valid Loss: 1.5015 | Valid Acc: 0.7205\n",
            "Validation loss improved from 1.5404 to 1.5015. 체크포인트를 저장합니다.\n",
            "Epoch: 11/30 | Train Loss: 0.4663 | Train Acc: 0.8862 | Valid Loss: 1.4800 | Valid Acc: 0.7250\n",
            "Validation loss improved from 1.5015 to 1.4800. 체크포인트를 저장합니다.\n",
            "Epoch: 12/30 | Train Loss: 0.3952 | Train Acc: 0.8982 | Valid Loss: 1.4668 | Valid Acc: 0.7278\n",
            "Validation loss improved from 1.4800 to 1.4668. 체크포인트를 저장합니다.\n",
            "Epoch: 13/30 | Train Loss: 0.3459 | Train Acc: 0.9064 | Valid Loss: 1.4647 | Valid Acc: 0.7288\n",
            "Validation loss improved from 1.4668 to 1.4647. 체크포인트를 저장합니다.\n",
            "Epoch: 14/30 | Train Loss: 0.3060 | Train Acc: 0.9132 | Valid Loss: 1.4653 | Valid Acc: 0.7301\n",
            "Epoch: 15/30 | Train Loss: 0.2780 | Train Acc: 0.9170 | Valid Loss: 1.4736 | Valid Acc: 0.7302\n",
            "Epoch: 16/30 | Train Loss: 0.2567 | Train Acc: 0.9213 | Valid Loss: 1.4755 | Valid Acc: 0.7292\n",
            "Epoch: 17/30 | Train Loss: 0.2365 | Train Acc: 0.9235 | Valid Loss: 1.4864 | Valid Acc: 0.7324\n",
            "Epoch: 18/30 | Train Loss: 0.2189 | Train Acc: 0.9262 | Valid Loss: 1.4975 | Valid Acc: 0.7306\n",
            "Epoch: 19/30 | Train Loss: 0.2082 | Train Acc: 0.9275 | Valid Loss: 1.5094 | Valid Acc: 0.7320\n",
            "Epoch: 20/30 | Train Loss: 0.1987 | Train Acc: 0.9289 | Valid Loss: 1.5082 | Valid Acc: 0.7328\n",
            "Epoch: 21/30 | Train Loss: 0.1895 | Train Acc: 0.9299 | Valid Loss: 1.5250 | Valid Acc: 0.7322\n",
            "Epoch: 22/30 | Train Loss: 0.1839 | Train Acc: 0.9300 | Valid Loss: 1.5321 | Valid Acc: 0.7323\n",
            "Epoch: 23/30 | Train Loss: 0.1780 | Train Acc: 0.9307 | Valid Loss: 1.5517 | Valid Acc: 0.7317\n",
            "Epoch: 24/30 | Train Loss: 0.1748 | Train Acc: 0.9310 | Valid Loss: 1.5554 | Valid Acc: 0.7301\n",
            "Epoch: 25/30 | Train Loss: 0.1708 | Train Acc: 0.9317 | Valid Loss: 1.5675 | Valid Acc: 0.7316\n",
            "Epoch: 26/30 | Train Loss: 0.1667 | Train Acc: 0.9319 | Valid Loss: 1.5752 | Valid Acc: 0.7318\n",
            "Epoch: 27/30 | Train Loss: 0.1638 | Train Acc: 0.9323 | Valid Loss: 1.5810 | Valid Acc: 0.7320\n",
            "Epoch: 28/30 | Train Loss: 0.1622 | Train Acc: 0.9324 | Valid Loss: 1.5924 | Valid Acc: 0.7289\n",
            "Epoch: 29/30 | Train Loss: 0.1579 | Train Acc: 0.9326 | Valid Loss: 1.6125 | Valid Acc: 0.7314\n",
            "Epoch: 30/30 | Train Loss: 0.1568 | Train Acc: 0.9323 | Valid Loss: 1.6230 | Valid Acc: 0.7316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 결과 Epoch 14이후로 Valid Loss 계속해서 증가"
      ],
      "metadata": {
        "id": "vhM7tAI3h6Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 로드\n",
        "model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n",
        "\n",
        "# 모델을 device에 올립니다.\n",
        "model.to(device)\n",
        "\n",
        "# 검증 데이터에 대한 정확도와 손실 계산\n",
        "val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)\n",
        "\n",
        "print(f'Best model validation loss: {val_loss:.4f}')\n",
        "print(f'Best model validation accuracy: {val_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlvfsFy0hfB5",
        "outputId": "8b489810-6e19-4221-e898-bb83fd601423"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model validation loss: 1.4647\n",
            "Best model validation accuracy: 0.7288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "가장 성능이 좋았던 모델을 로드"
      ],
      "metadata": {
        "id": "Pq9UGSfmiOJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tar_vocab['<sos>'])\n",
        "print(tar_vocab['<eos>'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGd-NsmniMXz",
        "outputId": "01d54512-0c4d-48ba-9b5b-81d19e50bdcd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. seq2seq 기계 번역기 동작시키기**"
      ],
      "metadata": {
        "id": "NLtl6XrliVK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련과정과 테스트 과정의 동작방식이 다름\n",
        "\n",
        "따라서 테스트과정에서의 모델을 다시 작성해주어야함"
      ],
      "metadata": {
        "id": "SaRrSYeQiiYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "전체적인 번역 단계를 정리하면 다음과 같음\n",
        "\n",
        "1) 번역하고자 하는 입력 문장이 인코더로 입력되어 인코더의 마지막 시점의 은닉 상태와 셀 상태를 얻음\n",
        "\n",
        "2) 인코더의 은닉 상태와 셀 상태, 그리고 토큰 <sos>를 디코더로 보냄\n",
        "\n",
        "3) 디코더가 토큰 <eos>가 나올 때까지 다음 단어를 예측하는 행동을 반복"
      ],
      "metadata": {
        "id": "JCozwEZziovY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_src = {v: k for k, v in src_vocab.items()}\n",
        "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
        "\n",
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_src(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0):\n",
        "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "  return sentence\n",
        "\n",
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_tar(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0 and encoded_word != tar_vocab['<sos>'] and encoded_word != tar_vocab['<eos>']):\n",
        "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "  return sentence\n"
      ],
      "metadata": {
        "id": "lS-8kqXdiSp3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder_input_test[25])\n",
        "print(decoder_input_test[25])\n",
        "print(decoder_target_test[25])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWAqXIebicUb",
        "outputId": "c85f4a03-3b70-4727-fdb2-991e751406ea"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[36  4 71  7  5  0  0]\n",
            "[  3  24  59  10 117   7   0   0   0   0   0   0   0   0   0   0]\n",
            "[ 24  59  10 117   7   4   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, max_output_len, int_to_src_token, int_to_tar_token):\n",
        "    encoder_inputs = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # 인코더의 초기 상태 설정\n",
        "    hidden, cell = model.encoder(encoder_inputs)\n",
        "\n",
        "    # 시작 토큰 <sos>을 디코더의 첫 입력으로 설정\n",
        "    # unsqueeze(0)는 배치 차원을 추가하기 위함.\n",
        "    decoder_input = torch.tensor([3], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    decoded_tokens = []\n",
        "\n",
        "    # for문을 도는 것 == 디코더의 각 시점\n",
        "    for _ in range(max_output_len):\n",
        "        output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
        "\n",
        "        # 소프트맥스 회귀를 수행. 예측 단어의 인덱스\n",
        "        output_token = output.argmax(dim=-1).item()\n",
        "\n",
        "        # 종료 토큰 <eos>\n",
        "        if output_token == 4:\n",
        "            break\n",
        "\n",
        "        # 각 시점의 단어(정수)는 decoded_tokens에 누적하였다가 최종 번역 시퀀스로 리턴합니다.\n",
        "        decoded_tokens.append(output_token)\n",
        "\n",
        "        # 현재 시점의 예측. 다음 시점의 입력으로 사용된다.\n",
        "        decoder_input = torch.tensor([output_token], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    return ' '.join(int_to_tar_token[token] for token in decoded_tokens)\n"
      ],
      "metadata": {
        "id": "-VFTUCBRix5F"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_train[seq_index]\n",
        "  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
        "  print(\"번역문장 :\",translated_text)\n",
        "  print(\"-\"*50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OrYUH33i3dO",
        "outputId": "3762553d-6760-4853-f093-c0be209a71f0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력문장 : who s that person ? \n",
            "정답문장 : qui est cette personne ? \n",
            "번역문장 : qui est ce type ?\n",
            "--------------------------------------------------\n",
            "입력문장 : you re very upset . \n",
            "정답문장 : vous etes tres contrarie . \n",
            "번역문장 : tu es tres contrariee .\n",
            "--------------------------------------------------\n",
            "입력문장 : do you like salmon ? \n",
            "정답문장 : est ce que vous aimez le saumon ? \n",
            "번역문장 : aimes tu le saumon ?\n",
            "--------------------------------------------------\n",
            "입력문장 : tom had to go . \n",
            "정답문장 : tom a du partir . \n",
            "번역문장 : tom a du partir .\n",
            "--------------------------------------------------\n",
            "입력문장 : can you see it ? \n",
            "정답문장 : peux tu le voir ? \n",
            "번역문장 : peux tu le voir ?\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_test[seq_index]\n",
        "  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
        "  print(\"번역문장 :\",translated_text)\n",
        "  print(\"-\"*50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH0YPHHpi5Zg",
        "outputId": "cc18b439-6ae8-4a97-f902-6921a6757840"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력문장 : how nice ! \n",
            "정답문장 : c est du joli ! \n",
            "번역문장 : comme c est chouette !\n",
            "--------------------------------------------------\n",
            "입력문장 : tom looks curious . \n",
            "정답문장 : tom semble curieux . \n",
            "번역문장 : tom a l air curieux .\n",
            "--------------------------------------------------\n",
            "입력문장 : i can handle it . \n",
            "정답문장 : je peux m en sortir . \n",
            "번역문장 : je peux y arriver .\n",
            "--------------------------------------------------\n",
            "입력문장 : stop worrying . \n",
            "정답문장 : arrete de t inquieter . \n",
            "번역문장 : arrete de jouer .\n",
            "--------------------------------------------------\n",
            "입력문장 : it s a pipe . \n",
            "정답문장 : c est une pipe . \n",
            "번역문장 : c est une pipe .\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}